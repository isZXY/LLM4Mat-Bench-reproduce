import os,sys
import json
import time
import torch
from datetime import datetime, timezone, timedelta
import pdb
import pandas as pd
import re
from typing import List, Dict
from tqdm import tqdm
from vllm import LLM, SamplingParams
sys.path.append(os.path.join(os.path.dirname(__file__), './llmprop_and_matbert'))
from create_args_parser import *

LOG_FILE = "/public/home/sjtu_zhuxuanyu/LLM4Mat-Bench/inference_run_all.log"

def log_print(msg):
    """æ‰“å°ä¿¡æ¯å¹¶å†™å…¥æ—¥å¿—æ–‡ä»¶"""
    print(msg)
    cst = timezone(timedelta(hours=8))  # ä¸­å›½æ ‡å‡†æ—¶é—´ UTC+8
    timestamp = datetime.now(cst).strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {msg}\n")

def extract_ans_from_chat_llm(result):
    """ä»å¤§æ¨¡å‹è¾“å‡ºä¸­æå–å¤§æ‹¬å·å†… JSON å†…å®¹"""
    start_index = result.find("{")
    end_index = result.find("}")
    if start_index != -1 and end_index != -1 and end_index > start_index:
        json_content = result[start_index:end_index + 1]
        return json_content.strip()
    else:
        return result.strip()

def parse_llama_prompt_to_messages(llama_prompt: str) -> List[Dict]:
    """
    å°† Llama é£æ ¼çš„é¢„æ¸²æŸ“ Prompt å­—ç¬¦ä¸²è§£æä¸ºæ ‡å‡†çš„ List[Dict] æ¶ˆæ¯æ ¼å¼ã€‚

    Args:
        llama_prompt: åŒ…å« <s>, [INST], <<SYS>>, <</SYS>>, [/INST] ç­‰æ ‡è®°çš„å­—ç¬¦ä¸²ã€‚

    Returns:
        æ ‡å‡†çš„ Hugging Face æ¶ˆæ¯åˆ—è¡¨æ ¼å¼ï¼š
        [
            {"role": "system", "content": "..."},
            {"role": "user", "content": "..."}
        ]
    """
    # 1. å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    # è¿™ä¸ªæ¨¡å¼ç”¨äºæ•è· <<SYS>> å’Œ <</SYS>> ä¹‹é—´çš„ SYSTEM å†…å®¹
    system_pattern = r'<<SYS>>\s*(.*?)\s*<</SYS>>'
    
    # 2. åŒ¹é…å¹¶æå– SYSTEM å†…å®¹
    # re.DOTALL ä½¿å¾— . èƒ½å¤ŸåŒ¹é…æ¢è¡Œç¬¦
    system_match = re.search(system_pattern, llama_prompt, re.DOTALL)
    
    system_content = ""
    if system_match:
        # æ¸…ç†æ•è·åˆ°çš„å†…å®¹ä¸­çš„å¤šä½™ç©ºç™½ç¬¦
        system_content = system_match.group(1).strip()
    
    # 3. æå– USER å†…å®¹
    # USER å†…å®¹ä½äº SYSTEM å—ä¹‹åï¼Œ[/INST] æ ‡è®°ä¹‹å‰
    # å…ˆæ‰¾åˆ° <<SYS>> å—çš„ç»“æŸä½ç½®ï¼Œç„¶åä»é‚£é‡Œå¼€å§‹æŸ¥æ‰¾ [/INST]
    
    # ç§»é™¤ system block å’Œ inst/sys tokens
    # r"(\[INST\].*?\[/INST\])" æ•è·æ•´ä¸ª INST/SYS å—
    inst_block_pattern = r"\[INST\]\s*(.*?)\s*\[/INST\]"
    inst_block_match = re.search(inst_block_pattern, llama_prompt, re.DOTALL)
    
    user_content = ""
    if inst_block_match:
        # æ•è· INST å’Œ /INST ä¹‹é—´çš„æ‰€æœ‰å†…å®¹
        inst_content = inst_block_match.group(1).strip()
        
        # ä» INST å—å†…å®¹ä¸­ç§»é™¤ SYSTEM å—ï¼Œå‰©ä¸‹çš„å°±æ˜¯ USER å†…å®¹
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç†æ²¡æœ‰ SYSTEM å—çš„æƒ…å†µ
        if system_content:
            # ä½¿ç”¨ re.escape æ¥ç¡®ä¿ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚<<, >>ï¼‰è¢«æ­£ç¡®åŒ¹é…
            cleaned_system_content = re.escape(f"<<SYS>>\n{system_content}\n<</SYS>>")
            # ç§»é™¤ system blockï¼Œstrip() æ¸…ç†ä¸¤ä¾§ç©ºç™½
            user_content = re.sub(cleaned_system_content, '', inst_content, flags=re.DOTALL).strip()
        else:
            # å¦‚æœæ²¡æœ‰ system blockï¼ŒINST å—å†…å®¹å°±æ˜¯ user content
            user_content = inst_content
            
        # ç§»é™¤ Llama çš„èµ·å§‹ token <s> (å¦‚æœå­˜åœ¨)
        if user_content.startswith('<s>'):
             user_content = user_content[3:].strip()
            
    # 4. æ„é€ æ ‡å‡† messages åˆ—è¡¨
    messages: List[Dict] = []
    if system_content:
        messages.append({"role": "system", "content": system_content})
    
    if user_content:
        messages.append({"role": "user", "content": user_content})
        
    return messages
def write_jsonl_line(where_to_save, record):
    """å°†å•æ¡ç»“æœå†™å…¥ JSONL æ–‡ä»¶"""
    with open(where_to_save, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    
    os.chdir("/public/home/sjtu_zhuxuanyu/LLM4Mat-Bench/code")


    # ==== æ£€æŸ¥ GPU ====
    if torch.cuda.is_available():
        device = torch.device("cuda")
        log_print(f"ğŸ”§ Number of available devices: {torch.cuda.device_count()}")
        log_print(f"Current device is: {torch.cuda.current_device()}")
        log_print(f"Training and testing on {torch.cuda.device_count()} GPUs!")
        log_print("-" * 50)
    else:
        log_print("âš ï¸ No GPU available, running on CPU.")
        device = torch.device("cpu")



    # ==== å‚æ•° ====
    args = args_parser()
    config = vars(args)
    
    dataset_name = config.get('dataset_name') 
    input_type = config.get('input_type') # description, structure, or composition
    prompt_type = config.get('prompt_type') # 'few_shot'( see five examples) /zero-shot
    max_len = config.get('max_len')
    property_name = config.get("property_name") # property name in dataset
    model_path = config.get("model_path")
    results_path = config.get("results_path")
    model_name = os.path.basename(model_path)
    batch_size = config.get("batch_size")

    os.makedirs(results_path, exist_ok=True)
    save_path = f"{results_path}/{model_name}_test_stats_for_{property_name}_{input_type}_{prompt_type}_{max_len}.json"



    # ==== è½½å…¥æ•°æ® ====
    data_path = f"../data/{dataset_name}/{dataset_name}_inference_prompts_data.csv"
    log_print(f"ğŸ“‚ Loading data from {data_path}")
    data = pd.read_csv(data_path)
    data = data.dropna(subset=[property_name])
    prompt_col = f"{property_name}_{input_type}_{prompt_type}"
    prompts_raw = list(data[prompt_col])

    # ==== å°†llamaæ ¼å¼promptsè½¬æ¢ä¸ºhuggingfaceæ ‡å‡†æ ¼å¼ ====
    prompts: List[List[Dict]] = []
    for llama_prompt in prompts_raw:
        # ç¡®ä¿ llama_prompt æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if pd.isna(llama_prompt):
            continue
            
        # è°ƒç”¨è§£æå‡½æ•°
        parsed_messages = parse_llama_prompt_to_messages(str(llama_prompt))
        
        if parsed_messages:
            prompts.append(parsed_messages)
        else:
            log_print(f"âš ï¸ Warning: Could not parse prompt:\n{llama_prompt[:100]}...")

    
    log_print(f"âœ… Successfully parsed {len(prompts)} prompts into standard messages format.")

        

    # ==== åˆå§‹åŒ– vLLM ====
    log_print(f"ğŸš€ Loading model from {model_path} ...")
    llm = LLM(
        model=model_path,
        tensor_parallel_size=torch.cuda.device_count(),  
        dtype="bfloat16",  
        trust_remote_code=True,
        gpu_memory_utilization=0.9,
        enforce_eager=False,
    )

    # ==== æ–°å¢ï¼šè·å– Tokenizer å¯¹è±¡===
    try:
        tokenizer = llm.get_tokenizer()
        log_print("ğŸ”§ Successfully retrieved tokenizer for manual template application.")
    except Exception as e:
        log_print(f"âŒ Error retrieving tokenizer: {e}. Cannot manually apply chat template.")
        sys.exit(1)


    ## è®¾ç½®æ¨¡å‹ç‰¹å®šé‡‡æ ·å‚æ•°
    # model_basename = os.path.basename(model_path)
    sampling_params = SamplingParams(
        temperature=0.7,
        top_k=10, 
        top_p=1,
        max_tokens=256,
    )


    # ==== æ¨ç† ====
    log_print("ğŸ§  Start inference ...")
    start_time = time.time()

    total_prompts = len(prompts)
    num_batches = (total_prompts + batch_size - 1) // batch_size

    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡å·²å®Œæˆéƒ¨åˆ†
    completed = 0
    if os.path.exists(save_path):
        with open(save_path, "r", encoding="utf-8") as f:
            completed = sum(1 for _ in f)
        log_print(f"â© Found existing file with {completed} completed samples, resuming...")

    for batch_idx in tqdm(range(completed // batch_size, num_batches), desc="Inference Progress", ncols=100):
        start = batch_idx * batch_size
        end = min(start + batch_size, total_prompts)
        batch_prompts = prompts[start:end]


        # 2. *** æ ¸å¿ƒä¿®æ”¹ï¼šæ‰‹åŠ¨æ¸²æŸ“ messages ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ ***
        batch_prompts_strings = []
        for messages in batch_prompts:
            # ä½¿ç”¨ tokenizer çš„ apply_chat_template è¿›è¡Œæ¸²æŸ“
            rendered_prompt = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True # å¿…é¡»æ·»åŠ ï¼Œä»¥æŒ‡ç¤ºæ¨¡å‹å¼€å§‹ç”Ÿæˆ
            )
            batch_prompts_strings.append(rendered_prompt)


        try:
            
            
            # # ğŸ” æ‰“å°åº”ç”¨æ¨¡æ¿åçš„å­—ç¬¦ä¸² (è¿™æ˜¯ vLLM è¦æ±‚çš„è¾“å…¥æ ¼å¼)
            # log_print("ğŸ” ==== Prompt Preview (åº”ç”¨æ¨¡æ¿åçš„å­—ç¬¦ä¸²) ====")
            # if batch_prompts_strings:
            #     log_print(f"[Prompt {start}] ----------------------------------")
            #     log_print(batch_prompts_strings[0])
            # # pdb.set_trace()

            outputs = llm.generate(batch_prompts_strings, sampling_params)
        except Exception as e:
            log_print(f"âŒ Error during batch {batch_idx}: {e}")
            continue

        for i, output in enumerate(outputs):
            response_text = output.outputs[0].text if len(output.outputs) > 0 else ""
            clean_result = extract_ans_from_chat_llm(response_text)

            record = {
                "response": clean_result
            }

            write_jsonl_line(save_path, record)

        # æ˜¾ç¤ºè¿›åº¦
        done = end / total_prompts * 100
        log_print(f"âœ… Completed {end}/{total_prompts} ({done:.1f}%)")


    end_time = time.time()
    elapsed = end_time - start_time
    log_print(f"\nğŸ¯ Inference completed in {elapsed/60:.2f} minutes.")
    log_print(f"Results saved to: {save_path}")